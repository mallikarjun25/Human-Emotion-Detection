{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a153699b-9ee8-46c6-b8c8-3c0f7cf3f092",
   "metadata": {},
   "source": [
    "#### Instructions:  \n",
    "1. Libraries allowed: **Python basic libraries, numpy, pandas, scikit-learn (only for data processing), pytorch, and ClearML.**\n",
    "2. Show all outputs.\n",
    "3. Submit jupyter notebook and a pdf export of the notebook. Check canvas for detail instructions for the report. \n",
    "4. Below are the questions/steps that you need to answer. Add as many cells as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb379a-fa53-49b6-8e63-1e9c6e005011",
   "metadata": {},
   "source": [
    "## Step 4: hyperparameter tuning without learning rate decay\n",
    "Do hyperparater tuning with ClearML and copy the plots (e.g., parallel coordinates) from ClearML and visualize them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2995dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=1c991bf1927f46c992a9ae8addc6f892\n",
      "2024-11-26 11:29:54,200 - clearml.Repository Detection - WARNING - Password protected Jupyter Notebook server was found! Add `sdk.development.jupyter_server_password=<jupyter_password>` to ~/clearml.conf\n",
      "2024-11-26 11:29:54,229 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/cc76c0e26e4f47f99330259096e45abd/experiments/1c991bf1927f46c992a9ae8addc6f892/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_layers': 3,\n",
       " 'filters': [32, 64, 128],\n",
       " 'learning_rate': [0.05, 0.08, 0.11],\n",
       " 'batch_size': 128,\n",
       " 'num_epochs': 5,\n",
       " 'momentum': [0.7, 0.8],\n",
       " 'weight_decay': 0.001}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from clearml import Task\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize ClearML Task\n",
    "task = Task.init(project_name=\"Hyperparameter Tuning\", task_name=\"With Learning Rate Scheduler\")\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparams = {\n",
    "    \"num_layers\": 3,\n",
    "    \"filters\": [32, 64, 128],\n",
    "    \"learning_rate\": [0.05, 0.08, 0.11],  # Refined learning rates\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 5,  # Increased epochs\n",
    "    \"momentum\": [0.7, 0.8],\n",
    "    \"weight_decay\": 0.001\n",
    "}\n",
    "task.connect(hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08e1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('train', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=hyperparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=hyperparams[\"batch_size\"], shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e444d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9106b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation function with learning rate scheduler\n",
    "def train_and_validate(lr, momentum):\n",
    "    model = CNN(num_classes=7).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3  # Early stopping patience\n",
    "    no_improvement = 0  # Track epochs without improvement\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        # Log metrics to ClearML\n",
    "        task.get_logger().report_scalar(\"Loss\", \"Validation\", val_loss, epoch)\n",
    "        task.get_logger().report_scalar(\"Accuracy\", \"Validation\", accuracy, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{hyperparams['num_epochs']}, \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "            no_improvement = 0  # Reset patience counter\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ebe78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LR=0.05, Momentum=0.7\n",
      "Epoch 1/5, Train Loss: 1.7343, Val Loss: 1.6478, Val Accuracy: 0.3382\n",
      "Epoch 2/5, Train Loss: 1.6073, Val Loss: 1.5494, Val Accuracy: 0.3945\n",
      "Epoch 3/5, Train Loss: 1.5161, Val Loss: 1.4922, Val Accuracy: 0.4232\n",
      "Epoch 4/5, Train Loss: 1.4387, Val Loss: 1.4139, Val Accuracy: 0.4532\n",
      "Epoch 5/5, Train Loss: 1.3774, Val Loss: 1.3928, Val Accuracy: 0.4699\n",
      "Training with LR=0.05, Momentum=0.8\n",
      "Epoch 1/5, Train Loss: 1.7248, Val Loss: 1.6468, Val Accuracy: 0.3304\n",
      "Epoch 2/5, Train Loss: 1.5663, Val Loss: 1.5340, Val Accuracy: 0.4000\n",
      "Epoch 3/5, Train Loss: 1.4481, Val Loss: 1.4368, Val Accuracy: 0.4443\n",
      "Epoch 4/5, Train Loss: 1.3744, Val Loss: 1.3652, Val Accuracy: 0.4711\n",
      "Epoch 5/5, Train Loss: 1.3100, Val Loss: 1.3441, Val Accuracy: 0.4796\n",
      "Training with LR=0.08, Momentum=0.7\n",
      "Epoch 1/5, Train Loss: 1.7128, Val Loss: 1.6555, Val Accuracy: 0.3607\n",
      "Epoch 2/5, Train Loss: 1.5524, Val Loss: 1.5047, Val Accuracy: 0.4056\n",
      "Epoch 3/5, Train Loss: 1.4462, Val Loss: 1.4718, Val Accuracy: 0.4276\n",
      "Epoch 4/5, Train Loss: 1.3696, Val Loss: 1.3687, Val Accuracy: 0.4786\n",
      "Epoch 5/5, Train Loss: 1.3168, Val Loss: 1.3226, Val Accuracy: 0.4930\n",
      "Training with LR=0.08, Momentum=0.8\n",
      "Epoch 1/5, Train Loss: 1.7080, Val Loss: 1.5911, Val Accuracy: 0.3819\n",
      "Epoch 2/5, Train Loss: 1.5274, Val Loss: 1.4816, Val Accuracy: 0.4162\n",
      "Epoch 3/5, Train Loss: 1.4121, Val Loss: 1.4392, Val Accuracy: 0.4411\n",
      "Epoch 4/5, Train Loss: 1.3453, Val Loss: 1.3699, Val Accuracy: 0.4685\n",
      "Epoch 5/5, Train Loss: 1.2848, Val Loss: 1.3083, Val Accuracy: 0.4997\n",
      "Training with LR=0.11, Momentum=0.7\n",
      "Epoch 1/5, Train Loss: 1.7050, Val Loss: 1.6608, Val Accuracy: 0.3582\n",
      "Epoch 2/5, Train Loss: 1.5281, Val Loss: 1.4482, Val Accuracy: 0.4403\n",
      "Epoch 3/5, Train Loss: 1.4075, Val Loss: 1.4010, Val Accuracy: 0.4599\n",
      "Epoch 4/5, Train Loss: 1.3335, Val Loss: 1.3293, Val Accuracy: 0.4897\n",
      "Epoch 5/5, Train Loss: 1.2756, Val Loss: 1.2969, Val Accuracy: 0.5078\n",
      "Training with LR=0.11, Momentum=0.8\n",
      "Epoch 1/5, Train Loss: 1.6940, Val Loss: 1.7492, Val Accuracy: 0.3556\n",
      "Epoch 2/5, Train Loss: 1.5011, Val Loss: 1.4341, Val Accuracy: 0.4420\n",
      "Epoch 3/5, Train Loss: 1.3770, Val Loss: 1.4289, Val Accuracy: 0.4485\n",
      "Epoch 4/5, Train Loss: 1.3005, Val Loss: 1.4096, Val Accuracy: 0.4509\n",
      "Epoch 5/5, Train Loss: 1.2534, Val Loss: 1.3274, Val Accuracy: 0.4822\n",
      "2024-11-26 11:57:14,467 - clearml.frameworks - INFO - Found existing registered model id=be5cc66bfe034b988e9331534c37e653 [C:\\Users\\smitp\\sem3\\DL\\Project 1\\Project 1\\best_model.pth] reusing it.\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for lr in hyperparams[\"learning_rate\"]:\n",
    "    for momentum in hyperparams[\"momentum\"]:\n",
    "        print(f\"Training with LR={lr}, Momentum={momentum}\")\n",
    "        best_model = train_and_validate(lr, momentum)\n",
    "\n",
    "# Save the best model to a file\n",
    "torch.save(best_model.state_dict(), \"best_model.pth\")\n",
    "print(\"Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90713db-1714-42d9-b9f1-6ae79ef13fcb",
   "metadata": {},
   "source": [
    "## Step 5: hyperparameter tuning with learning rate decay\n",
    "Do hyperparater tuning with ClearML and copy the plots (e.g., parallel coordinates) from ClearML and visualize them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3484a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Password protected Jupyter Notebook server was found! Add `sdk.development.jupyter_server_password=<jupyter_password>` to ~/clearml.conf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=958c04ab27bb4669bf1c2c0268b74847\n",
      "ClearML results page: https://app.clear.ml/projects/cc76c0e26e4f47f99330259096e45abd/experiments/958c04ab27bb4669bf1c2c0268b74847/output/log\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "from clearml import Task\n",
    "import torch\n",
    "\n",
    "# Close any existing ClearML task\n",
    "if Task.current_task():\n",
    "    Task.current_task().close()\n",
    "\n",
    "# Initialize new ClearML task\n",
    "task = Task.init(project_name=\"Hyperparameter Tuning\", task_name=\"With Learning Rate Decay\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cac662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training loop to include scheduler and save the best model\n",
    "def train_and_validate_with_lr_decay(lr, momentum, weight_decay, step_size, gamma):\n",
    "    model = CNN(num_classes=7).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3  # Stop if no improvement for 3 epochs\n",
    "    no_improvement = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(hyperparams[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{hyperparams['num_epochs']}, \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "            no_improvement = 0  # Reset patience counter\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improvement >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Save the best model after training\n",
    "    if best_model is not None:\n",
    "        torch.save(best_model.state_dict(), \"best_model_with_lr_decay.pth\")\n",
    "        print(\"Best model saved as 'best_model_with_lr_decay.pth'.\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b752add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 1.7282, Val Loss: 1.7214, Val Accuracy: 0.2997\n",
      "Epoch 2/5, Train Loss: 1.6042, Val Loss: 1.5485, Val Accuracy: 0.4094\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n",
      "Epoch 3/5, Train Loss: 1.5031, Val Loss: 1.4590, Val Accuracy: 0.4383\n",
      "Epoch 4/5, Train Loss: 1.4352, Val Loss: 1.4285, Val Accuracy: 0.4500\n",
      "Epoch 5/5, Train Loss: 1.3802, Val Loss: 1.4039, Val Accuracy: 0.4573\n",
      "Best model saved as 'best_model_with_lr_decay.pth'.\n",
      "Epoch 1/5, Train Loss: 1.7217, Val Loss: 1.6522, Val Accuracy: 0.3316\n",
      "Epoch 2/5, Train Loss: 1.5593, Val Loss: 1.5488, Val Accuracy: 0.3976\n",
      "Epoch 3/5, Train Loss: 1.4617, Val Loss: 1.4946, Val Accuracy: 0.4173\n",
      "Epoch 4/5, Train Loss: 1.3973, Val Loss: 1.4150, Val Accuracy: 0.4664\n",
      "Epoch 5/5, Train Loss: 1.3424, Val Loss: 1.3754, Val Accuracy: 0.4751\n",
      "Best model saved as 'best_model_with_lr_decay.pth'.\n",
      "Epoch 1/5, Train Loss: 1.7059, Val Loss: 1.6219, Val Accuracy: 0.3736\n",
      "Epoch 2/5, Train Loss: 1.5635, Val Loss: 1.5231, Val Accuracy: 0.4053\n",
      "Epoch 3/5, Train Loss: 1.4561, Val Loss: 1.4473, Val Accuracy: 0.4422\n",
      "Epoch 4/5, Train Loss: 1.3847, Val Loss: 1.3776, Val Accuracy: 0.4639\n",
      "Epoch 5/5, Train Loss: 1.3232, Val Loss: 1.3431, Val Accuracy: 0.4843\n",
      "Best model saved as 'best_model_with_lr_decay.pth'.\n",
      "Epoch 1/5, Train Loss: 1.7065, Val Loss: 1.6179, Val Accuracy: 0.3649\n",
      "Epoch 2/5, Train Loss: 1.5286, Val Loss: 1.4777, Val Accuracy: 0.4302\n",
      "Epoch 3/5, Train Loss: 1.4130, Val Loss: 1.3977, Val Accuracy: 0.4511\n",
      "Epoch 4/5, Train Loss: 1.3397, Val Loss: 1.3778, Val Accuracy: 0.4721\n",
      "Epoch 5/5, Train Loss: 1.2967, Val Loss: 1.3199, Val Accuracy: 0.4944\n",
      "Best model saved as 'best_model_with_lr_decay.pth'.\n",
      "Epoch 1/5, Train Loss: 1.7063, Val Loss: 1.6218, Val Accuracy: 0.3555\n",
      "Epoch 2/5, Train Loss: 1.5313, Val Loss: 1.5312, Val Accuracy: 0.4166\n",
      "Epoch 3/5, Train Loss: 1.4157, Val Loss: 1.4991, Val Accuracy: 0.4030\n",
      "Epoch 4/5, Train Loss: 1.3424, Val Loss: 1.3546, Val Accuracy: 0.4781\n",
      "Epoch 5/5, Train Loss: 1.2877, Val Loss: 1.3594, Val Accuracy: 0.4826\n",
      "Best model saved as 'best_model_with_lr_decay.pth'.\n",
      "Epoch 1/5, Train Loss: 1.6977, Val Loss: 1.6195, Val Accuracy: 0.3650\n",
      "Epoch 2/5, Train Loss: 1.5094, Val Loss: 1.4790, Val Accuracy: 0.4187\n",
      "Epoch 3/5, Train Loss: 1.4017, Val Loss: 1.3784, Val Accuracy: 0.4711\n",
      "Epoch 4/5, Train Loss: 1.3311, Val Loss: 1.3383, Val Accuracy: 0.4817\n",
      "Epoch 5/5, Train Loss: 1.2913, Val Loss: 1.3397, Val Accuracy: 0.4876\n",
      "Best model saved as 'best_model_with_lr_decay.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning loop\n",
    "for lr in hyperparams[\"learning_rate\"]:\n",
    "    for momentum in hyperparams[\"momentum\"]:\n",
    "        best_model = train_and_validate_with_lr_decay(\n",
    "            lr, momentum, hyperparams[\"weight_decay\"], step_size=5, gamma=0.5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e76f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), \"best_model_with_lr_decay.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166213c-84de-43ca-8630-b501ea97fa41",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation\n",
    "Evaluate the best model on test dataset and report accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6429170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4926\n",
      "Precision: 0.4871\n",
      "Recall: 0.4150\n",
      "F1 Score: 0.4056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.ImageFolder('test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Recreate the model and load the saved best model weights\n",
    "best_model = CNN(num_classes=7).to(device)  # Instantiate the model\n",
    "best_model.load_state_dict(torch.load(\"best_model_with_lr_decay.pth\"))  # Load the saved weights\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Evaluate the model\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)  # Use the best_model here\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083c618-85f3-4241-86bd-eaa12999c19c",
   "metadata": {},
   "source": [
    "## Step 7: Analysis\n",
    "Provide a complete analysis of the whole process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e4c87-92ed-47b6-bb37-d69af4fd98e4",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d7c4e",
   "metadata": {},
   "source": [
    "## Part-1\n",
    "\n",
    "### Objective:\n",
    "The project aimed to develop a robust pipeline for emotion detection using CNNs, focusing on data preprocessing, model training, and hyperparameter tuning with ClearML.\n",
    "\n",
    "#### Data Preprocessing:\n",
    "\n",
    "Normalized dataset (Mean: 0.5077, Std: 0.2119).\n",
    "Applied augmentations (rotation, flipping) to improve generalization.\n",
    "Partitioned data into training, validation, and test sets.\n",
    "#### Model Training:\n",
    "\n",
    "Developed a DynamicCNN with configurable layers and regularization (dropout, weight decay).\n",
    "Used SGD and Adam optimizers, with learning rate decay to enhance convergence.\n",
    "Overfitted a small subset (two samples) to validate model capacity.\n",
    "#### Hyperparameter Tuning:\n",
    "\n",
    "Tested learning rates, momentum, and weight decay.\n",
    "Optimal parameters identified: Learning rate: 0.05–0.08, Weight decay: 1e-4.\n",
    "ClearML facilitated experiment tracking and visualization.\n",
    "#### Performance:\n",
    "\n",
    "DynamicCNN achieved Validation Accuracy: 33.56% after 10 epochs.\n",
    "Overfitting on two samples reached 100% accuracy in one epoch.\n",
    "### Challenges:\n",
    "Overfitting in smaller datasets.\n",
    "Validation accuracy plateaued, requiring more data or advanced architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0caed53",
   "metadata": {},
   "source": [
    "## Part-2\n",
    "### Objective:\n",
    "Develop a CNN for multi-class emotion detection with PyTorch and ClearML for hyperparameter tuning.\n",
    "\n",
    "#### Data Preparation:\n",
    "\n",
    "Dataset split into training, validation, and test sets.\n",
    "Applied transformations: grayscale conversion, resizing, normalization, and augmentations (flipping, rotation).\n",
    "#### Model Development:\n",
    "\n",
    "A configurable CNN with 2 convolutional layers, ReLU, max-pooling, and fully connected layers.\n",
    "Optimized using SGD with momentum and weight decay.\n",
    "#### Hyperparameter Tuning:\n",
    "\n",
    "Tested learning rates (0.05, 0.08, 0.11), momentum (0.7, 0.8), and weight decay (0.001).\n",
    "Best configuration: LR=0.11, Momentum=0.7, Weight Decay=0.001.\n",
    "#### Performance:\n",
    "\n",
    "Validation Accuracy: ~50%.\n",
    "Test Metrics:\n",
    "Accuracy: 49.26%, Precision: 48.71%, Recall: 41.50%, F1 Score: 40.56%.\n",
    "\n",
    "### Strengths:\n",
    "ClearML enabled effective experiment tracking.\n",
    "Learning rate decay improved convergence.\n",
    "Achieved moderate performance with a simple CNN.\n",
    "\n",
    "### Challenges:\n",
    "Limited accuracy (~49%) indicates potential class imbalance or insufficient model complexity.\n",
    "Variability in precision and recall suggests underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a1016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
